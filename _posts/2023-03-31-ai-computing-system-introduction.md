---
layout:       post
title:        "AI Computing System Intruction"
author:       "galaxies"
header-style:  text
catalog:      true
tags:
    - computing
    - AI
    - gpt3
    - RL
 
---

# AI è®¡ç®—ä½“ç³»ä¸ŽçŸ©é˜µè¿ç®—


## AIèŠ¯ç‰‡æŒ‡æ ‡

## ç®—åŠ›å•ä½

![](/img/in-post/post-ai/hardware/Unit-of-Arithmetic.png)

1.  OPS
    1.  OPS
    2.  OPS/W
2. MACs
   1. Multiply-Accumulate Operationsï¼Œä¹˜åŠ ç´¯è®¡æ“ä½œã€‚1MACsåŒ…å«ä¸€ä¸ªä¹˜æ³•æ“ä½œä¸Žä¸€ä¸ªåŠ æ³•æ“ä½œï¼Œ ~2FLOPsï¼Œé€šå¸¸MACsä¸ŽFLOPså­˜åœ¨ä¸€ä¸ª2å€çš„å…³ç³»ã€‚

3. FLOPs
   1. Floating Point Operationsï¼Œæµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼Œç”¨æ¥è¡¡é‡æ¨¡åž‹è®¡ç®—å¤æ‚åº¦ï¼Œå¸¸ç”¨ä½œç¥žç»ç½‘ç»œæ¨¡åž‹é€Ÿåº¦ çš„é—´æŽ¥è¡¡é‡æ ‡å‡†ã€‚
   2. FLOPs = æ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•° Ã— æµ®ç‚¹è¿ç®—çš„æ“ä½œæ•°
   å…¶ä¸­ï¼Œæ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•°è¡¨ç¤ºè®¡ç®—æœºæ¯ç§’é’Ÿå¯ä»¥æ‰§è¡Œçš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼Œæµ®ç‚¹è¿ç®—çš„æ“ä½œæ•°è¡¨ç¤ºå•ä¸ªæµ®ç‚¹è¿ç®—æ‰€éœ€è¦çš„åŸºæœ¬æ“ä½œæ•°ã€‚
       * å‡è®¾ä¸€å°è®¡ç®—æœºæ¯ç§’é’Ÿå¯ä»¥æ‰§è¡Œ100äº¿æ¬¡æµ®ç‚¹è¿ç®—ï¼ˆå³10^10 FLOPsï¼‰ï¼Œè€Œå•ä¸ªæµ®ç‚¹è¿ç®—éœ€è¦æ‰§è¡Œ5ä¸ªåŸºæœ¬æ“ä½œï¼ˆå¦‚åŠ ã€å‡ã€ä¹˜ã€é™¤ç­‰ï¼‰ï¼Œåˆ™è¯¥è®¡ç®—æœºçš„æ¯ç§’é’Ÿçš„FLOPsæ•°ä¸ºï¼š
      FLOPs = 10^10 Ã— 5 = 50äº¿
      * å¯¹äºŽå·ç§¯å±‚è€Œè¨€ï¼ŒFLOPsçš„è®¡ç®—å…¬å¼å¦‚ä¸‹: ð¹ð¿ð‘‚ð‘ƒð‘ =2(ð»(ð‘Š(ð¶#$ (ð¾(ð¾(ð¶%&'
   
4. Memory Access Cost
   1. Memory Access Costï¼Œå†…å­˜å ç”¨é‡ï¼Œç”¨æ¥è¯„ä»·æ¨¡åž‹åœ¨è¿è¡Œæ—¶çš„å†…å­˜å ç”¨æƒ…å†µã€‚ 1ð‘¥1 å·ç§¯FLOPs ä¸º 2 â‹… ð» â‹… ð‘Š â‹… ð¶!" ( ð¶#$% , å…¶å¯¹åº”MACä¸º:
ð»(ð‘Š( ð¶in +Cout' +(ð¶in âˆ—ð¶out')


## Key Metrics â€“ AIèŠ¯ç‰‡å…³é”®æŒ‡æ ‡
1. ç²¾åº¦ Accuracy
   * è®¡ç®—ç²¾åº¦ (FP32/FP16 etc.) 
   * æ¨¡åž‹ç»“æžœç²¾åº¦ (ImageNet 78%)
2. åžåé‡ Throughoutput
   * é«˜ç»´å¼ é‡å¤„ç† (high dimension tensor) 
   * å®žæ—¶æ€§èƒ½ (30 fps or 20 tokens)
3. æ—¶å»¶ Latency 
4. èƒ½è€— Energy
   1. IOT è®¾å¤‡æœ‰é™çš„ç”µæ± å®¹é‡
   2. æ•°æ®ä¸­å¿ƒæ¶²å†·ç­‰å¤§èƒ½è€—
5. ç³»ç»Ÿä»·æ ¼ System Cost
   1. ç¡¬ä»¶è‡ªèº«çš„ä»·æ ¼ 
      1. ç‰‡å†…å¤šçº§ç¼“å­˜ Cache å¤§å° >> å†…å­˜è®¾è®¡
      2. PE æ•°é‡ã€èŠ¯ç‰‡å¤§å°ã€çº³ç±³åˆ¶ç¨‹ >> ç”µè·¯è®¾è®¡
   2. ç³»ç»Ÿé›†æˆä¸Šä¸‹æ¸¸å…¨æ ˆç­‰æˆæœ¬
6. æ˜“ç”¨æ€§ Flexibility
   1. è¡¡é‡å¼€å‘æ•ˆçŽ‡å’Œå¼€å‘éš¾åº¦
   2. å¯¹ä¸»æµAIæ¡†æž¶æ”¯æŒåº¦ (PyTorch) >> è½¯ä»¶æ ˆ

## å¯¹äºŽAIåŠ é€Ÿçš„å…³é”®è®¾è®¡ç‚¹
1. æå‡åžåé‡ Increase Throughput å’Œé™ä½Žå»¶æ—¶ Reduce Latency
2. ä½Žå»¶æ—¶ Low Latency å’Œ Batch Size ä¹‹é—´ Tradeoff
3. ç¡¬ä»¶è°ƒåº¦æ€è€ƒ
   1. åŽ»æŽ‰æ²¡æœ‰æ„ä¹‰çš„ MACs
      1. å¢žåŠ å¯¹ç¨€ç–æ•°æ®çš„ç¡¬ä»¶ç»“æž„ sparse data
   2. é™ä½Žæ¯æ¬¡ MAC çš„è®¡ç®—æ—¶é—´
      1. å¢žåŠ æ—¶é’Ÿé¢‘çŽ‡ clock frequency
      2. å‡å°‘æŒ‡ä»¤å¼€é”€ instruction overhead
4. PE, Processing Elements
   1. å¢žåŠ  PE çš„æ ¸å¿ƒæ•°é‡
      1. æ›´å¤šçš„MACså¹¶å‘(parallel)
      2. ä½¿ç”¨æ›´é«˜çº³ç±³åˆ¶ç¨‹ï¼Œå¢žåŠ PEçš„é¢ç§¯å¯†åº¦(areadensity)
   2. å¢žåŠ  PE çš„åˆ©ç”¨çŽ‡ utilization
      1.  å°†è®¡ç®—è´Ÿè½½å°½å¯èƒ½åˆ†é…åˆ°ä¸åŒPE(distributeworkload)
      2.  å‡è¡¡PEä¹‹é—´è®¡ç®—è´Ÿè½½(balanceworkloads)
      3.  åˆé€‚çš„å†…å­˜å¸¦å®½æœ‰æ•ˆé™ä½Žç©ºé—²æ—¶é’Ÿå‘¨æœŸ(reduceidlecycles)
![](/img/in-post/post-ai/hardware/process-elelment.png)


## è®¡ç®—æ€§èƒ½ä»¿çœŸ

1. éœ€è¦è€ƒè™‘çš„å°±æ˜¯æ•°æ®å¸¦å®½ä¸Žè®¡ç®—å¸¦å®½çš„æ¯”ä¾‹ã€‚

![](/img/in-post/post-ai/hardware/Computing%20performance%20simulation.png)

è¯¦ç»†å¯ä»¥å‚è€ƒï¼Œå‚è€ƒæ–‡çŒ®ã€‚


## çŸ©é˜µè¿ç®—(Matrix Multiplication)

1.  ä»Žå·ç§¯ Conv åˆ°çŸ©é˜µä¹˜ MM
  * é€šè¿‡æ•°æ®é‡æŽ’ï¼Œå®Œæˆ Im2col çš„æ“ä½œä¹‹åŽä¼šå¾—åˆ°ä¸€ä¸ªè¾“å…¥çŸ©é˜µï¼Œå·ç§¯çš„ Weights ä¹Ÿå¯ä»¥è½¬æ¢ä¸ºä¸€
 ä¸ªçŸ©é˜µï¼Œå·ç§¯çš„è®¡ç®—å°±å¯ä»¥è½¬æ¢ä¸ºä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜çš„æ±‚è§£ï¼Œå¾—åˆ°æœ€ç»ˆçš„å·ç§¯è®¡ç®—ç»“æžœã€‚
![](/img/in-post/post-ai/hardware/conv-to-mm-1.png)

     * åœ¨è®¡ç®— ð‘€ð‘…Ã—ð‘ð‘… å°å—æ—¶ï¼Œä¼ ç»Ÿçš„æ–¹æ³•æ˜¯åœ¨ K ç»´åº¦ä¸Šæ‹†åˆ†ï¼Œåœ¨ä¸€æ¬¡è®¡ç®— Kernel å¤„ç†ä¸­ï¼Œä»…è®¡ ç®— K ç»´çš„å±€éƒ¨ã€‚é‚£ä¹ˆåœ¨æ¯æ¬¡è®¡ç®— Kernel çš„å¤„ç†ä¸­ï¼Œéƒ½ä¼šå‘ç”Ÿå¯¹è¾“å‡ºçš„åŠ è½½å’Œå­˜å‚¨ã€‚

   ![](/img/in-post/post-ai/hardware/conv-to-mm.png)

   * çŸ©é˜µä¹˜åˆ†å— Tiling
  æ ¹æ® Cache å¤§å°æ¥å¯¹çŸ©é˜µè¿›è¡Œåˆ†å— Tilingï¼Œæœ€å¤§ç¨‹åº¦é‡ç”¨æ•°æ®å’Œåˆ©ç”¨ç©ºé—´æ¢æ—¶é—´


2. CPU/GPU æ”¯æŒçŸ©é˜µä¹˜çš„åº“
   * å®žçŽ°é€»è¾‘:
      * Lib æ„ŸçŸ¥ç›¸ä¹˜çŸ©é˜µçš„ Shape
      * é€‰æ‹©æœ€ä¼˜çš„ Kernel å®žçŽ°æ¥æ‰§è¡Œ
   * å®žçŽ°æ–¹æ³•
     * Loop å¾ªçŽ¯ä¼˜åŒ– (Loop tiling)
     * å¤šçº§ç¼“å­˜ (memory hierarchy)
   * çŽ°æœ‰åº“ï¼ˆMatrix Multiplication (GEMM)
     * CPU: OpenBLAS, Intel MKL, etc.
     * GPU: cuBLAS, cuDNN, etc.

![](/img/in-post/post-ai/hardware/implement-mm.png)

3. å·ç§¯ä»£æ›¿ç®—æ³•
   1. Fast Fourier Transform
   2. Strassen
   3. Winograd 

## æ¯”ç‰¹ä½å®½(Bits Width)
1. é‡åŒ–åŽŸç†
   1. æ¨¡åž‹é‡åŒ–æ¡¥æŽ¥äº†å®šç‚¹ä¸Žæµ®ç‚¹ï¼Œå»ºç«‹äº†ä¸€ç§æœ‰æ•ˆçš„æ•°æ®æ˜ å°„å…³ç³»ï¼Œä½¿å¾—ä»¥è¾ƒå°çš„ç²¾åº¦æŸå¤±ä»£ä»·èŽ· å¾—äº†è¾ƒå¥½çš„æ”¶ç›Š

2. bit Witdh æ˜¯ä»€ä¹ˆ
* Floating Point(FP): allows range to change for each value(E-bits)
* Fixed Point(Int): has fixed range

![](/img/in-post/post-ai/hardware/bit-width.png)

* CPU æˆ–è€… GPU é»˜è®¤é‡‡ç”¨ FP32
   
1. é™ä½Žç²¾åº¦çš„å¥½å¤„
   1. å¯¹äºŽ MAC çš„è¾“å…¥å’Œè¾“å‡ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ•°æ®çš„æ¬è¿å’Œå­˜å‚¨å¼€é”€
      1. SmallerMemory->Lowerenergy
   2. å‡å°‘ MAC è®¡ç®—çš„å¼€é”€å’Œä»£ä»·
      1.  int8 x int8 -> int16;fp16 x fp16 -> fp32
   3. é™ä½Žä½å®½å¯¹åŠŸè€—å’ŒèŠ¯ç‰‡é¢ç§¯çš„å½±å“

2. ä»€ä¹ˆå†³å®šæ¯”ç‰¹ä½å®½ Bit Width ?
   1. è®­ç»ƒ Training: weights, activation, partial sums,gradients and weight update.
   2. æŽ¨ç† Inference: weights, activations, partial sums

3. åšæ³•
   1. å¯¹ç²¾åº¦çš„å½±å“ Impact on Accuracy
      1.  éœ€è¦è€ƒè™‘ä¸åŒæ•°æ®é›†(NLP/CV)ã€ä¸åŒä»»åŠ¡
      2.  ä¸åŒç½‘ç»œæ¨¡åž‹ä¹‹é—´çš„å·®å¼‚è¿›è¡Œæµ‹è¯„ (e.g., classification > detection)
   2. è®­ç»ƒå’ŒæŽ¨ç†çš„æ•°æ®ä½å®½
      1. 32bit float å¯ä»¥ä½œä¸ºå¼±åŸºçº¿;
      2. å¯¹äºŽè®­ç»ƒä½¿ç”¨ FP16ã€BF16ã€TF32;
      3. æŽ¨ç† CV ä»»åŠ¡ä»¥ int8 ä¸ºä¸»ï¼ŒNLP ä»¥ FP16ä¸ºä¸»ï¼Œå¤§æ¨¡åž‹ int8/FP16 æ··åˆ;


## å‚è€ƒæ–‡ä»¶

1.  > [ Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices ]
   (https://arxiv.org/pdf/1807.07928.pdf) 

2. > [ DeepLearningSystem]
   (https://github.com/chenzomi12/DeepLearningSystem) 
   



