---
layout:       post
title:        "How to design infra smartly"
author:       "galaxies"
header-style:  text
catalog:      true
tags:
    - AI Infra
    - Cloud Computing
    - HPC
---

# AI 时代
在这个 AI 时代，语言模型已经成为了人机交流的关键工具。而 ChatGPT 则是其中的佼佼者，这个由 OpenAI 训练的模型，以其卓越的理解和生成能力，成为了一个人人景仰的网红。

## ChatGPT 并不是黑科技，是持续开放科研的产物
ChatGPT 背后的技术，最主要的一篇文章是 2022 年 OpenAI 发表的论文 InstructGPT 。InstructGPT 的核心思路是之前两条研究线路所带来的：一个是自然语言理解的大规模语言模型 LLM，另一个是带人类反馈的增强学习 RLHF。

大规模语言模型 LLM 在前面几年方兴未艾，从 GPT 开始，往回可以推到 Bert ，这两种都是基于所谓的 Transformer 结构来设计的。而Transformer 的出现本身又是为了解决早期的序列模型（比如说LSTM 和 RNN）的问题所提出来的。。很有意思的是这一系列模型多少都采取了不带太强结构的统计方法：“根据周边的词语来预测中间的词语”，或者“根据前面的文字来生成后面的词语”。这和传统的基于语法树的方法很不一样，感兴趣考古的读者可以去看看 PCFG，计算语言学当中很经典的一个算法。

RLHF 也是一个近年以来比较流行的算法。增强学习最经典的书应该是 Sutton & Barto 所写的同名著作《Reinforcement Learning》。2004年，Pieter Abbeel 和吴恩达就利用 RL 提出了叫做 Apprenticeship Learning 的方法，来让机器学会复杂的动作，比如说让直升机进行空中转体：

2017年开始DeepMind 的一系列工作（电子游戏、围棋等）让 RL 深入人心，ChatGPT 对于对话系统的训练也深得前面这些工作的影响。因此，整体而言，ChatGPT的一系列工作，都在前面有着很深的铺垫，应该说是站在开放科研的肩膀上做出的工作，其中的功底不得不让人叹服。这不是别人做出大模型之后，简单跟进说“我们可以做得更大”，而是在原有的基础上做更多创新的成果。

## ChatGPT 是工程、产品的胜利

OpenAI 前序所推出的 playground、GPT-3 API 等等，一边在进行产品和市场的适配的途中，另一方面也给后续的科研带来了大量的数据输入。根据 InstructGPT 的文章披露，当时 OpenAI 雇佣了约 40 名左右的标注人员来提供手工写的文字；这个数字在最近披露的报道中上升到了 1000 名左右。计算机领域有一个短语叫做 human in the loop，将一篇科研文章变成一个prototype，然后再将用户的体验、数据的回流、标注、再训练这个闭环做得非常精准，ChatGPT 在这一个领域当中体现出了高超的工程能力。

工程和产品体验会给 ChatGPT 的下一代带来更大优势。试想，一亿人每个月在给 ChatGPT 生成对话数据训练下一代模型，这是现在任何一个研究院，包括一线大厂，所无法企及的。

## ChatGPT 不会让人失业，反而会带来更多的机会

    一个广为人知的故事是，达芬奇在创作《岩间圣母》的时候，很多背景部分不是他画的 - 这些简单重复的地方就让他的助手画了。今天 ChatGPT 就是助手，当内容创作者能够花更少的时间做重复劳动的时候，创新会变得更多 - 这是历史上多次证明的。

## Infrastructure 会是这一场仗当中的赢家，但是要聪明地设计Infra。
硅谷著名风投 A16Z 在最近一篇对于 AIGC 的文章当中提到那么一句话：“目前看基础设施提供商是这个市场当中最大的赢家”。

不过要做这个赢家，就要更加聪明地设计 infra 才行。AI 计算不同于传统上所说的“云计算”，而更加接近于我们所说的“高性能计算” HPC。

* 云计算很多时候在关注资源的池化和虚拟化
    1. 怎么把计算，存储，网络，从物理资源变成虚拟的概念，“批发转零售”；
    2. 如何在这种虚拟环境下把利用率做上去，或者说超卖；
    3. 怎么更加容易地部署软件，做复杂软件的免运维（比如说，容灾、高可用）等等，不一而足。
* 当今的AI 的训练
    1. 并不要求特别强的虚拟化。一般训练会“独占”物理机，除了简单的例如建立虚拟网络并且转发包之外，并没有太强的虚拟化需求。
    2. 需要很高性能和带宽的存储和网络。例如，网络经常需要几百 G 以上的 RDMA 带宽连接，而不是常见的云服务器几 G 到几十 G 的带宽。
    3. 对于高可用并没有很强的要求，因为本身很多离线计算的任务，不涉及到容灾等问题。
    4. 没有过度复杂的调度和机器级别的容灾。因为机器本身的故障率并不很高（否则 GPU 运维团队就该去看了），同时训练本身经常以分钟级别来做 checkpointing，在有故障的时候可以重启整个任务从前一个 checkpoint 恢复。

    也就是说，对于 AI 的用户而言而言，尤其是今天那么大规模的训练，性能和规模是第一位的，传统云服务所涉及到的一些能力，是第二位的。

    今天不少的 AI 软硬件设计，都依然透出着高性能计算的影子。例如，阿里云在 2022 年提出的飞天智算集群“灵骏”，通过 GPU 的高速互联以及轻量级的平台 PAI，来管理万卡级别的AI计算的需求；微软 Azure 在云上提供了这样一个专为高性能 AI 设计的机型：8xA100 GPU + 8x200G Infiniband。Meta 在 2022 年公布了自己万卡数量的科研集群 RSC：这些产品的设计都是明显为了高性能 AI 计算来提供的。

    对于提供基础设施的供应商来说，AI 计算是一个新的机会，也是一个关键的时机，需要重新审视长期提供通用云服务而形成的思维模式。

    ## AI 计算未来可期

    AI 领域永远不缺惊喜，原以为计算机视觉已经走到了尽头，忽然 AIGC 柳暗花明又一村；原以为 Stable Diffusion 已经审美疲劳，忽然 ChatGPT 又打开无数的应用。

    最后没什么可说的了，作为一直战斗在 AI platform 一线的老兵，用 Richard Sutton的一句话来做结语：

    The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.

    Richard Sutton："The Bitter Lesson"


## 参考文件

1. >[ ChatGPT，和聪明地设计 Infra ]
   (http://scholarsupdate.hi2net.com/news.asp?NewsID=33603) 
