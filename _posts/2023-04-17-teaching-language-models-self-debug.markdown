---
layout:     post
title:      "Teaching Large Language Models to Self-Debug"
subtitle:   " Teaching Large Language Models to Self-Debug"
date:       2023-04-17 22:24:00
author:     "Galaxies"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - LLM
    - Self-Debug
---

> 机器在没有任何代码执行的正确性或者error_messages,该模型可以通过自然语言解释生成的代码来识别其错误。这意味着计算机编译原理中的解释器，有可能被自然语言所接管，这就进一步意味这人类可以通过自然语言来编程。
> 这个跟以前让机器生成代码然后出错把错误发给机器的方式还不同，是一种让机器去解释代码，从理解代码本身来修正错误。
> 最大的感触，如果想训练好这个东西，一定要把代码，错误，以及解释写的很规范，只有这样才有机会让机器学习你的方式，又一次提出了一个致命性的问题，中文世界的代码质量到底如何呢？
> “Yeah It's on. ”
>
>  

## ABSTRACT

大型语言模型（LLMs）在代码生成方面取得了令人印象深刻的性能和表现。然而，对于复杂的编程任务，一次性生成正确的解决方案变得具有挑战性。因此，一些先前的工作已经设计了程序修复方法来提高代码生成性能。在这项工作中，我们提出了SELF-DEBUGGING，它通过少量的演示来教导大型语言模型调试其预测的程序。特别是，我们证明了SELF-DEBUGGING可以教大型语言模型进行橡皮鸭式的调试；也就是说，在没有任何关于代码正确性或错误信息的反馈的情况下，该模型能够通过用自然语言解释生成的代码来识别其错误。

自我调试在几个代码生成基准上达到了最先进的性能，包括文本到SQL的Spider数据集生成，C++到Python的翻译的TransCoder和文本到Python的生成的MBPP。在Spider基准测试中，没有单元测试来验证预测的正确性。在这个基准中，带有代码解释的SELF-DEBUGGING持续地将基准线提高了2-3%，并将最难的标签问题的预测准确性提高了9%。在TransCoder和MBPP这两个有单元测试的基准上，SELF-DEBUGGING将基线精度提高了12%。

同时，通过利用反馈信息和重复使用失败的预测，SELF-DEBUGGING明显提高了采样效率，并可以匹配或超过基线模型，生成超过10倍的候选程序。

## 正文

代码生成是一个长期存在的挑战，其应用包括从自然语言中合成代码[63, 8, 2, 32]，通过实例编程[14, 5, 11]，以及代码翻译[44, 10]。特别是，最近的大型语言模型已经显示出比以前的深度神经网络有了明显的飞跃，与之前的深度神经网络相比，有了很大的改进[8, 13, 38, 67, 57]。然而，对于许多编程任务来说，一次尝试就能生成正确的代码是具有挑战性的。受以下观察结果的启发，当从模型中抽取多个程序时，正确的代码更有可能被预测出来[8, 13, 32]。因此，有一项工作设计了重新排序技术，以从多个样本中选择最佳候选人。通常需要几十个样本才能从中选择最佳候选程序。

直观地说，即使对于人类程序员来说，也不能保证第一次写的代码总是准确的。人类通常不会完全丢弃不正确的代码，而是研究代码并调查执行结果，然后进行修改以解决执行问题。
观察代码并调查执行结果，然后进行修改以解决执行中的错误。因此，先前的工作提出了深度学习技术来修复预测的代码，这些技术在各种编码上表现出明显的性能提升。
在各种编码任务上表现出明显的性能提升[20, 53, 17, 6]。然而，这些方法需要对代码修复模型进行额外的训练。虽然最近的一些工作表明，大型语言模型有可能产生反馈信息，以批判和完善其完善其输出的潜力[50, 35, 28, 36, 3]，但之前的工作表明在缺乏外部反馈的情况下，这种大型语言模型还没有能力纠正代码、如单元测试或人类指令。

在这项工作中，我们提出了SELF-DEBUGGING，即我们教大型语言模型通过少量的提示来调试。我们通过少量的提示教大型语言模型调试它自己预测的代码。不需要任何额外的模型训练，SELFDEBUGGING指示模型执行代码，然后根据代码和执行结果生成反馈信息。
代码和它的执行结果生成反馈信息。与之前利用人类反馈进行代码修复的工作不同、在那里，反馈信息解释了代码错误和如何修复它们[6, 2]，SELF-DEBUGGING教导模型通过代码解释来识别执行错误。这个调试过程这种调试过程让人想起人类程序员的橡皮鸭子调试，用自然语言向橡皮鸭子解释代码的线段，可以在没有专家指导的情况下显著提高调试效率。

我们在GPT-3模型系列[8]中的code-davinci-002上评估SELF-DEBUGGING。SELFDEBUGGING在不同类型的代码生成任务上实现了最先进的性能、包括文本到SQL的生成、代码翻译和文本到Python的生成。

![](/img/in-post/post-ai/application/Self-Debugging.png)
使用大型语言模型进行迭代调试的SELF-DEBUGGING。在每个调试步骤中，模型首先生成新的代码，然后执行代码，模型解释
代码。代码解释和执行结果构成了反馈信息。然后被送回模型以执行更多的调试步骤。当单元测试不可用时、反馈可以是纯粹基于代码解释的。

### 代码生成的提示（Prompting for Code Generation）

* Few-shot prompting.

少量提示的目的是指示语言模型解决一个有几个输入输出演示的任务。几次输入输出的演示[4]。以文本到SQL的生成为例，少量的提示
提示将感兴趣的问题与一个（问题，SQL）对的列表放在一起，这样当模型被要求预测给定提示的后续标记时，它将遵循提示的格式来生成SQL查询。图2展示了一个提示的示例。除了输入-输出的示范之外、我们可以选择在提示中添加一个指令，以提供一个高层次的任务描述[41, 46, 51]。例如，在图3所示的我们的SELF-DEBUGGING提示的前两个步骤中，这两个提示都以要求模型产生解释的指令开始。我们在附录中提供了完整的几个镜头的提示在附录中。

![](/img/in-post/post-ai/application/problem-1.png)

![](/img/in-post/post-ai/application/answer-1.png)
图二，一个文本到SQL生成的范例。该问题取自Spider数据集[63]。问题描述包含数据库结构，模型需要预测SQL查询。提示包括每张表中的一条记录的内容

![](/img/in-post/post-ai/application/prompt-question-2.png)

![](/img/in-post/post-ai/application/self-debugging-with-explanation.png)
图三，一个文本到SQL生成的SELF-DEBUGGING提示的例子。该模型是
需要预测代码、解释和反馈。图中省略了数据库信息
为了清晰起见，我们在附录A中介绍了完整的提示。

* Execution-based code selection.

先前的工作表明，对多个样本进行解码可以显著提高大型语言模型的性能[54, 49]。特别是，对于代码
生成任务，我们可以利用代码执行来选择最终的预测[11, 32, 49, 65, 37]。一个其中一项工作是利用执行结果的多数票来选择最终预测[11, 32, 49]，而其他工作则是设计重新排序方案。而其他工作则设计了重新排序的方案来提高性能[65, 37, 61, 64]。在这项工作中、
当有多个预测时，我们遵循第一条工作路线，在有多个预测的代码中选择具有在那些没有遇到执行错误的代码中选择执行频率最高的代码，然后应用对该代码进行自调试。一些代码生成任务伴随着单元测试来指定程序的执行行为[8, 2, 32, 22]。具体来说，单元测试是一组输入输出对{(ik, ok)}。Kk=1，并且当P(ik)=ok, ∀k∈{1, ..., K}时，程序P通过单元测试。当单元测试出现在当问题描述中出现单元测试时，我们会在执行基于执行的多数票之前，过滤掉那些没有通过单元测试的程序。基于执行的多数投票。


### 自我调试框架（SELF-DEBUGGING Framework）
图1展示了我们用于迭代调试的SELF-DEBUGGING框架，其中我们利用了预训练的大型语言模型而不进行微调。考虑到问题的描述，该模型首先预测候选程序，然后推断程序的正确性并产生反馈信息用于后续的调试步骤。当反馈信息指出当反馈信息指出预测是正确的，或者达到允许的最大调试次数时，调试过程结束。
现有的工作表明，语言模型可以被训练来理解人类的反馈并根据指令进行修改[2, 38, 6]。然而，目前还不清楚语言模型是否能够在没有人类帮助的情况下自行调试。在下文中，我们将讨论不同类型的反馈信息，这些信息可以通过代码执行和少量提示自动获得和生成。

* SELF-DEBUGGING with Simple Feedback
  最简单的自动反馈形式是一个句子，它只是表明代码的正确性，而没有更多的细节信息。更详细的信息。例如，在文本到SQL的生成过程中，少量的提示提供了以下反馈信息反馈信息 "上面的SQL预测是正确的！" 对于所有正确的SQL查询，以及 "上面的SQL预测是错误的。上面的SQL预测是错误的。对于错误的预测，"请修正SQL"。
* SELF-DEBUGGING with Unit Tests (UT)
对于问题描述包括单元测试的代码生成任务，除了利用代码执行来检查代码的正确性外，我们还可以在反馈中展示执行结果。
执行来检查代码的正确性，我们还可以在反馈信息中显示执行结果。消息中，这为调试提供了更丰富的信息。图5展示了一个单元测试的样本代码翻译的反馈信息。直观地说，检查运行时错误信息和执行的执行结果也有助于人类程序员更有效地进行调试。在我们的实验中、
我们将证明，利用单元测试可以大大改善调试性能。
![](/img/in-post/post-ai/application/self-debugging-framework-1.png)
![](/img/in-post/post-ai/application/self-debugging-framework-2.png)

图五 自我调试提示代码翻译的例子。左对齐的块是模型预测，右对齐的块包含输入的C++代码和基于代码执行的反馈信息。
代码执行的反馈信息。我们在附录B中介绍了完整的提示信息。

  
* SELF-DEBUGGING via Code Explanation (Expl.)

尽管最近取得了有希望的进展，表明大型语言模型可以产生批判，以避免有害的模型输出，并提高其在一些自然语言和其他领域的性能。
避免有害的模型输出[18, 3]，并提高他们在一些自然语言和推理任务上的表现[50, 28, 47]，但之前的工作还没有显示出模型生成的反馈在代码生成上的有效性[6]。另一方面，大型语言模型已被证明能够以文本[55, 29, 68]和代码[19, 9]的形式描述他们生成的问题解决方案。
在这些观察的启发下，我们没有教大型语言模型去预测错误信息、我们建议通过解释生成的代码来教模型进行自我调试。这种调试
这个调试过程让人联想到橡皮鸭调试，程序员通过对橡皮鸭逐行解释来调试代码程序员通过向橡皮鸭逐行解释来调试代码[24]。通过描述代码的实现并将其与问题描述进行比较，人类程序员通常能够在没有额外指导的情况下识别错误。根据经验，我们观察到一个大型的语言模型也可以从橡皮鸭的调试中获益。特别是在没有单元测试的情况下。

### 应用（Applications）

1. Text-to-SQL Generation

图三过程包含3个步骤。首先，我们提示模型总结问题并推断出问题所需的返回类型；即相应SQL查询的表列数量。
在第二步中，我们执行SQL查询，并将返回的表添加到模型输入中，以便于代码解释。生成的SQL解释包括每个条款的详细描述、
包括在返回表中的列数，以及完整的SQL查询的高级含义。查询。当返回的表有超过2行时，只有前2行被包括在提示中。
最后，该模型将推断出的SQL解释和问题描述进行比较，然后预测当前SQL查询的正确性。自我调试过程在SQL

2. Code Translation

其目标是将一种编程语言的代码翻译成另一种语言。

3. Text-to-Python Generation

## 实验

* 为给定任务训练的模型。
spider基准包含一个超过1万个样本的训练集。样本，而最先进的模型大多是在这个训*练集上进行微调的。我们将SELF-DEBUGGING与Graphix-T5[31]、T5-3B + Syn数据[66]和T5-3B + N-best Reranking[64]进行比较、它们都是由T5架构[42]改编的，并为文本到SQL的生成进行了专门训练。尽管LEVER[37]也利用code-davinci-002来生成候选SQL查询，但他们训练一个验证器来选择基于执行的最终预测，因此也需要额外的训练。

* 基于提示的方法。
我们将SELF-DEBUGGING与最近的一些方法进行比较，这些方法也只进行提示而不进行额外的训练。也只进行提示而不进行任何额外的训练。特别是MBR-Exec[49]和Coder-Reviewer[65]。和Coder-Reviewer[65]都首先通过提示预训练的大型语言模型。之后，MBR-Exec[49]会选择具有最常见的而Coder-Reviewer[65]则通过利用问题描述中预测代码的可能性来选择程序。而Coder-Reviewer[65]则通过利用问题描述中预测的代码的可能性来选择程序（Coder）。

---



## 结论

模型，简单的反馈仍然可以提高这两个应用的模型预测。对于所有任务，模型从更丰富的自调试反馈信号中获益。特别是代码
解释使模型能够在没有单元测试的情况下进行自我调试。



—— Galaxies

## 参考文献
1. >[Teaching Large Language Models to Self-Debug](https://arxiv.org/pdf/2304.05128.pdf)