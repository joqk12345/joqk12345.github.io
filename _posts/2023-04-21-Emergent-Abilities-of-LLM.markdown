---
layout:     post
title:      "Emergent Abilities of Large Language Models"
subtitle:   "Emergent Abilities of Large Language Models"
date:       2023-04-17 22:24:00
author:     "Galaxies"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - LLM
    - Emergent
---


> 大模型神乎其神的涌现能力，在本周分享中多次被提及，我们分析一下他目前研究的一些进展。
> “Yeah It's on. ”
>  
>  

## ABSTRACT
扩大语言模型的规模已被证明可以预测地提高性能和样本在广泛的下游任务中的效率。本文则讨论了一种不可预测的现象，我们称之为大型语言模型的涌现能力。我们认为一个如果一种能力在较小的模型中不存在，但在较大的模型中却存在，那么这种能力就是突发的。
因此，涌现能力不能简单地通过推断较小模型的性能来预测。小模型的性能来预测。这种涌现的存在提出了一个问题，即额外的缩放是否有可能进一步扩大语言模型的能力范围。


### 导论
最近几年，语言模型在自然语言处理（NLP）领域中引起了革命性变化。现在已经广泛认识到，增加语言模型的规模（如训练计算、模型参数等）可以提高下游NLP任务的性能和样本效率（Devlin等，2019年；Brown等，2020年等）。在许多情况下，规模对性能的影响通常可以通过缩放定律进行方法论预测，例如交叉熵损失的缩放曲线已被证明在实践中可以跨越七个数量级以上（Kaplan等，2020年；Hoffmann等，2022年）。另一方面，某些下游任务的性能表现出与规模的连续改进相矛盾的现象，这些任务无法提前预测（Ganguli等，2022年）。

在本文中，我们将讨论大型语言模型的涌现能力的不可预测现象。涌现能力作为一种理念，在物理、生物学和计算机科学等领域已经长期被讨论（Anderson，1972年；Hwang等，2012年；Forrest，1990年；Corradini和O'Connor，2010年；Harper和Lewis，2012年等）。
我们将考虑对涌现性的以下一般定义，该定义改编自斯坦哈特（2022年）并根植于1972年诺贝尔物理学奖获得者菲利普·安德森（Anderson，1972年）的一篇名为“更多是不同的”的论文：当一个系统中的定量变化导致行为的定性变化时，就会出现涌现性。
在这里，我们将探讨模型规模的涌现性，其通过训练计算和模型参数数量来衡量。具体而言，我们将大型语言模型的涌现能力定义为在小规模模型中不存在但在大规模模型中存在的能力，因此不能通过简单地对小规模模型的性能改进进行外推来预测（第2节）。我们调查了在各种先前工作中观察到的涌现能力，并将其分类到少样本提示（第3节）和增强提示策略（第4节）等设置中。涌现性激励未来研究为何会获得这些能力，以及更多的缩放是否会导致更多的涌现能力，这些问题被我们突出为该领域的重要问题（第5节）。

### 涌现能力的定义
作为一个广泛的概念，Emergence 经常被非正式地使用，并可以在很多不同的方式下合理地解释。在本文中，我们将考虑 Emergence 对大型语言模型的涌现能力的一个聚焦定义：如果一个能力不在较小规模的模型中出现，而在较大规模的模型中出现，则这是一个 Emergent 能力。Emergent 能力不会直接通过在小规模模型上外推缩放规律（即一致的性能改进）而被预测到。当通过一个缩放曲线可视化 Emergent 能力时（x 轴：模型规模，y 轴：性能），Emergent 能力呈现出明显的模式——性能在达到一定的关键规模阈值之前几乎是随机的，之后性能急剧提高到显著高于随机的水平。这种定性的变化也被称为相变——总体行为的显著变化，这是通过检查小规模系统无法预见到的（Huberman & Hogg，1987）。

今天的语言模型主要是沿着三个因素进行缩放的：计算量，模型参数数量和训练数据集的大小（Kaplan 等人，2020；Hoffmann 等人，2022）。在本文中，我们将通过在 x 轴上绘制每个模型的训练 FLOP 来分析缩放曲线（Hoffmann 等人，2022）。由于训练计算量更多的语言模型往往也具有更多的参数，我们还在附录 D 中展示了以模型参数数量为 x 轴的图表（请参见图 11 和图 12，以及图 4 和图 10）。使用训练 FLOPs 或模型参数作为 x 轴可以产生形状相似的曲线，这是因为大多数密集的 Transformer 语言模型族已经将训练计算量大致与模型参数成比例地缩放（Kaplan 等人，2020）。

训练数据集的大小也是一个重要因素，但我们不将能力与其绘制在同一张图上，因为许多语言模型系列对所有模型大小使用相同数量的训练示例(Brown等，2020; Rae等，2021; Chowdhery等，2022)。虽然我们在这里专注于训练计算和模型大小，但没有一个代理可以充分捕捉到规模的所有方面。例如，Chinchilla(Hoffmann等，2022)的参数数量只有Gopher(Rae等，2021)的四分之一，但使用类似的训练计算;而稀疏混合专家模型比密集模型具有更多的参数训练/推理计算(Fedus等，2021; Du等，2021)。总的来说，将涌现性视为许多相关变量的函数可能是明智的。例如，在图4中稍后我们还将将涌现性作为WikiText103迷惑度(Merity等，2016)的函数进行绘制，它恰好与Gopher/Chinchilla的训练计算密切相关(尽管这种相关性在长期内可能不成立)。

注意，能力首次出现涌现的尺度取决于许多因素，不是能力的不可变属性。例如，对于在更高质量数据上训练的模型，使用更少的训练计算或更少的模型参数也可以出现涌现性。相反，涌现性能力还关键地取决于其他因素，例如不受数据量、数据质量或模型参数数量限制。今天的语言模型可能没有被最优化地训练(Hoffmann等，2022)，我们对如何最好地训练模型的理解将随着时间的推移而发展。我们在本文中的目标不是表征或声称观察到涌现能力所需的特定规模，而是旨在讨论先前工作中涌现行为的例子。


### Few-Shot Prompted 任务

![](/img/in-post/post-ai/algo/fewshot-prompting.png)

我们首先讨论提示范式中的涌现能力，这是由GPT-3 (Brown et al., 2020)普及的。在提示中，预先训练的语言模型会接收任务的提示（例如自然语言指令），并在没有任何进一步训练或参数梯度更新的情况下完成响应。Brown等人（2020）提出了几次提示，其中在模型的上下文（输入）中包括一些输入-输出示例作为序言，然后要求模型为未见推理示例执行任务。图1显示了一个示例提示。

通过几次提示完成任务的能力是涌现的，当模型随机表现直到某个规模时，表现会大幅度超出随机水平。图2展示了五个语言模型系列中的八种涌现能力。

![](/img/in-post/post-ai/algo/model_scale.png)

八个在少许提示的情况下出现的例子。每个点都是一个独立的模型。当一个语言模型达到随机的性能，直到一定的规模后，性能显著提高到远远超过随机的水平时，通过几发提示执行任务的能力就会出现的性能，直到一定的规模，之后性能明显增加到远远超过随机。注意
注意，使用了更多训练计算的模型通常也有更多的参数--因此，我们显示了一个模型参数数量的类似数字。


###  Augmented Prompting Strategies(增强型的提示策略)

虽然Few-shot prompting可能目前是与大型语言模型交互的最常见方式，但最近的研究提出了几种其他提示和微调策略，以进一步增强语言模型的能力。如果一种技术与基准方法相比没有改进或者有害，直到应用到足够大规模的模型时，我们也将考虑该技术为涌现能力。

![](/img/in-post/post-ai/algo/Augmented_Prompting_Strategies.png)

*  Multi-step reasoning

推理任务，尤其是涉及多个步骤的任务，一直以来对于语言模型和NLP模型来说都很具有挑战性(Rae等，2021；Bommasani等，2021；Nye等，2021)。最近出现了一种称为“链式思维提示”的提示策略，使语言模型能够通过引导它们生成一系列中间步骤来解决这类问题，然后再给出最终答案(Cobbe等，2021；Wei等，2022b；Suzgun等，2022)。如图3A所示，只有当推向1023个训练FLOP(大约100B个参数)的时候，链式思维提示才能超越没有中间步骤的标准提示。在使用解释来补充最终答案的情况下，通过增加少量的提示，也能够得到类似的性能提升(Lampinen等，2022)。

* Instruction following

另一条不断增长的研究方向是旨在使语言模型通过阅读描述任务的说明书(而无需使用少量样本)来更好地执行新任务。通过对以说明书形式表述的任务的混合微调，已经证明语言模型能够适当地响应描述未知任务的说明(Ouyang等，2022；Wei等，2022a；Sanh等，2022；Chung等，2022)。如图3B所示，Wei等(2022a)发现这种指示微调技术对于小于7·1021个训练FLOP(8B个参数)的模型而言，反而会降低性能，只有当推向1023个训练FLOP(大约100B个参数)时才能提高性能(尽管Sanh等(2022)随后发现，这种遵循指示的行为也可以通过微调更小的编码器-解码器T5模型来引导)。


* Program execution

考虑到涉及多个步骤的计算任务，如大数加法或执行计算机程序。Nye等人（2021）表明，对语言模型进行微调以预测中间的
输出（"scratchpad"）使他们能够成功执行这种多步骤的计算。如图所示图3C，在8位数的加法运算中，使用scratchpad只对9-1019个训练FLOPs（40M参数）或更大的模型。

* Model calibration.

最后，部署语言模型研究的一个重要方向是校准、衡量模型是否能够预测他们将能够正确回答哪些问题。Kadavath等人（2022）比较了两种测量校准的方法：一种是真/假技术，即模型首先提出答案，然后评估其答案的概率 "P（True）"。答案，然后评估其答案正确的概率 "P(True)"，以及更标准的校准方法。它使用正确答案与其他答案选项相比的概率。如图3D所示，"真/假 "技术的优越性只有在扩展到最大的3-1023个训练FLOPs（52B参数）的最大模型规模时，真/假技术的优势才显现出来。

![](/img/in-post/post-ai/algo/emergent_scale.png)

## 讨论
我们已经看到，迄今为止，只有在足够大的语言模型上进行评估时，才会观察到一系列的能力--------几张照片的提示设置或其他方面。因此，它们的出现不能通过简单地推断小规模模型上的表现来预测。新出现的少量提示性任务也是不可预测的，因为这些任务并没有明确地包括在预训练中，而且我们可能 我们可能不知道语言模型可以执行的少量提示任务的全部范围。这就提出了一个问题 这就提出了这样一个问题：进一步的扩展是否有可能赋予更大的语言模型以新的出现的能力。语言模型目前无法完成的任务是未来出现的主要候选者；例如 例如，在BIG-Bench中有几十个任务，即使是最大的GPT-3和PaLM模型也没有 甚至最大的GPT-3和PaLM模型也不能达到高于随机的性能（见附录E.4）。

规模能够不可预测地启用新技术，这不仅仅是理论上的。考虑一下图2H所示的Word inContext（WiC）基准（Pilehvar & Camacho-Collados, 2019），作为一个历史例子。在这里，将GPT-3扩展到大约3-1023个训练FLOPs（175B参数）未能释放出高于随机的
3 关于这个负面的结果，Brown等人（2020年）提到了GPT-3的模型结构或使用的方法。关于这一负面结果，Brown等人（2020）认为GPT-3的模型结构或使用自回归语言建模目标（而不是使用去噪训练目标）作为潜在的原因，并建议训练一个具有可比性的双向结构的模型作为补救。双向结构的模型作为补救措施。然而，后来的工作发现，进一步扩展一个仅有解码器的语言模型实际上足以在这项任务上实现高于随机的性能。如图2H所示、将PaLM（Chowdhery等人，2022）从3-1023个训练FLOPs（62B参数）扩展到3-1024个训练FLOPs（540B参数）导致了性能的大幅跃升，而没有像Brown等人（2022）所建议的那样进行重大的架构调整。布朗等人(2020)建议的重大架构变化。

* Potential explanations of emergence

虽然有几十个关于突发能力的例子，但目前很少有令人信服的解释。为什么这种能力会以这样的方式出现。对于某些任务来说，可能有一些自然的直觉来解释为什么涌现需要一个大于特定阈值规模的模型。例如，如果一个多步骤的推理任务需要l个步骤的顺序计算
任务需要l个顺序计算的步骤，这可能需要一个至少有O (l)层深度的模型。同样合理的是，假设更多的参数和更多的训练能够更好地记忆4 例如，在闭卷答题中取得好成绩，可能需要更多的参数和训练，这对需要世界知识的任务有帮助。例如，在闭卷答题上的良好表现可能需要一个有足够参数的模型来捕捉压缩的知识础（尽管基于语言模型的压缩器可以比传统的压缩器有更高的压缩率压缩机(Bellard, 2021))。

* Beyond scaling
尽管我们可能观察到一种涌现的能力发生在某个尺度上，但这种能力有可能后来在更小的尺度上实现。换句话说，模型的规模并不是解锁突发能力的唯一因素。随着训练大型语言模型的科学的发展，某些能力可能会在较小的模型上以新的方式解锁。随着训练大型语言模型的科学发展，某些能力可能会被新的架构、更高质量的数据或改进的训练程序解锁给小型模型。

此外，一旦发现一种能力，进一步的研究可能会使这种能力用于更小规模的模型。考虑到使语言模型能够遵循自然语言指令的新生方向
描述一项任务（Wei等人，2022a；Sanh等人，2022；Ouyang等人，2022，除其他外）。虽然Wei et al.(2022a)最初发现，基于指令的微调只对68B参数或更大的纯解码器模型有效，但Sanh等人(2022)在一个具有编码器-解码器结构的11B模型中诱发了类似行为、这种结构在微调后的性能通常高于纯解码器结构（Wang等人，2022a）。作为另一个例子，Ouyang等人（2022）提出了一种针对InstructGPT的人类反馈的微调和强化学习方法。的反馈方法，这使得一个1.3B的模型在人类评分者的评估中超过了大得多的这使得一个13亿的模型在人类评测员的评估中，在广泛的使用案例中胜过更大的模型。

也有一些工作是关于改善语言模型的一般少量提示能力的（Gao等人，2021；Schick & Schütze，2021，等等）。理论和可解释性研究（Wei等人，2021a；Saunshi等人，2021），了解为什么语言建模目标会促进某些下游行为，这反过来又会对如何使涌现成为可能产生影响。反过来，对如何使涌现超越简单的缩放也有影响。例如，预训练数据的某些特征（如预训练数据的某些特征（例如，长程一致性，有许多罕见的类别）也被证明与涌现的少量提示，并有可能在较小的模型中实现它（Xie等人，2022；Chan等人、2022），而且在某些情况下，几率学习可能需要某些模型架构（Chan等人，2022）。计算语言学的工作进一步显示了训练数据的阈值频率如何能够激活
当模型参数和训练FLOPs保持不变时，出现的句法规则学习（Wei et al、2021b），这甚至被证明有惊人的 "aha "时刻，类似于心理语言学中的那些文献（Abend等人，2017；Zhang等人，2021）。随着我们继续训练语言模型，降低随着我们继续训练语言模型，降低涌现能力的规模阈值将变得更加重要，使这种能力的研究能够更广泛地提供给社区（Bommasani等人，2021；Ganguli等人，2022；Liang等人，2022）。

当然，一个只由规模增加（训练计算量、模型参数和数据集大小）的程序是有局限性的。参数和数据集大小）。例如，规模的扩大最终可能会受到硬件限制的瓶颈、而有些能力在这时可能还没有出现。其他能力可能永远不会出现--例如，任务例如，即使是在一个非常大的训练数据集的分布范围之外的任务，也可能永远不会取得任何显著的性能。最后，一种能力可能会出现，然后趋于平稳；换句话说，并不能保证换句话说，并不能保证通过扩展使一种能力达到理想的水平。

* Another view of emergence
特定任务能力的出现可以作为语言模型的函数来分析。诸如WikiText103（Merity等人，2016）这样的一般文本语料库上的困惑度来分析。总的来说，出现的能力可能应该被看作是许多相关变量的一个函数许多相关变量的功能。

* Emergent risks

需要考虑，偏见、毒性。一些这样的行为可能是后门漏洞、无意中的欺骗或有害的内容的合成。涉及数据过滤、预测、治理和自动发现的方法有害行为的方法已经被提出来，用于发现和减轻突发风险。

* Sociological changes

最后，这里讨论的涌现能力集中在模型行为上，只是NLP中几种涌现类型中的一种。在NLP中的涌现（Manning等人，2020；Teehan等人，2022）。另一种值得注意的质变类型是社会学的，在这种情况下，不断扩大的规模已经改变了社区对语言模型的看法和使用。例如
例如，NLP在历史上一直专注于特定任务的模型（Jurafsky & Martin, 2009）。最近，规模的扩大导致了对 "通用 "模型的研究和开发的爆发，因为它们是单一模型，旨在执行训练数据中没有明确编码的一系列任务（如GPT-3、Chinchilla和PaLM）（Manning，2022）。
在涌现的社会学转向通用模型的过程中，有一组关键的结果是当扩展使得少量提示的通用模型的性能优于先前由微调的特定任务模型保持的技术水平。

### Directions for future work
* Further model scaling
* Improved model architectures and training
* Better techniques for and understanding of prompting
* Frontier tasks

---



## 结论
我们已经讨论了语言模型的涌现能力，对于这些能力，有意义的表现只在一定的计算规模上被观察到。迄今为止，只有在一定的计算规模上观察到有意义的表现。新出现的能力可以跨越各种语言模型的范围、任务类型和实验场景。这种能力是最近发现的扩大语言模型规模的结果。而它们是如何出现的，以及更多的扩展是否会使更多的涌现能力出现似乎是NLP领域未来重要的研究方向。



—— Galaxies

## 参考文献
1. >[Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf)
2. >[https://zhuanlan.zhihu.com/p/621438653](https://zhuanlan.zhihu.com/p/621438653)