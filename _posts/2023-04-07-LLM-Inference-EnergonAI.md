---
layout:       post
title:        "Large-scale Model Inference-EnergonAI"
author:       "galaxies"
header-style:  text
catalog:      true
tags:
    - Inference System
    - 10-100Billion Parameter
    - Transformer Models
    - deployment
    - latency
    - throughput
    - memory constraint
    - distributed inference
---

# LLM 大模型部署的挑战

虽然模型规模已经扩大到万亿参数，但是由于延时，吞吐量和内存限制，100-1000亿参数规模的模型实际部署仍面临调整。

这篇论文提出了 EnergonAI，以解决在单个或多个 GPU 系统上高效部署 10-1000 亿参数 Transformer 模型的挑战。EnergonAI 采用层次控制器系统架构来协调多个设备并有效地支持不同的并行模式。

这篇论文主要是讲如何解决在单个或多个 GPU 系统上高效部署 10-1000 亿参数 Transformer 模型的挑战。

* 计算与内存的挑战

大型Transformer模型的确需要更多的计算和内存，因此需要采用不同于传统小型模型推理系统的附加内存管理和通信技术来扩展多GPU上的大型模型推理系统。然而，这些技术不能直接从大型模型训练系统移植过来。虽然推理是训练的前向过程，但模型训练主要针对高吞吐量，而模型推理则既针对吞吐量又针对延迟。因此，在它们之间存在着巨大的优化策略差距。更重要的是，现有的推理框架通常使用C++/CUDA高度耦合实现，并且很难为新模型定制，更不用说具有复杂内存管理和通信逻辑的分布式推理了。

虽然推理是训练的前向过程，但模型训练主要针对高吞吐量，而模型推理则既针对吞吐量又针对延迟。因此，在它们之间存在着巨大的优化策略差距。为了解决这个问题，可以使用优化技术，例如剪枝、量化或蒸馏，可以在不影响性能的情况下减少模型大小或复杂度。您还可以使用加速技术，例如并行化、缓存或批处理，可以加速模型执行或推理。

* 大模型推理与小模型推理的区别
   1. 首先，小型模型推理框架与定制的 C++/CUDA 内核实现高度耦合。为新的模型定制它们是不容易的
   2. 我们观察到，这些传统的优化方法，例如内核融合和定制的并行内核，对大型模型的影响很小
   3. 现有的大型Transformer模型推理系统利用了训练系统中的并行方法。虽然训练和推理都采用了类似的计算模式虽然训练和推理都有类似的计算模式，但模型训练主要以高吞吐量为目标，而模型推理则以吞吐量和延迟为目标。
    
    直接照搬训练策略不是最佳解决方案。为了弥补这一缺陷，我们认为必须解决大型模型推理的独特挑战。

* Transformer 与 模型并行范式
    有三种潜在的范式可以使Transformer推理并行，即数据并行、张量并行、和流水线并行。
    1. 对于数据并行，它可以多次复制一个模型进行多次复制，并且只能有利于吞吐量。
    2. 对于张量并行，它将一个转换层水平地分割成不同的设备。尽管它对互连的性能有很高的要求对互连的性能要求很高，但它有可能同时优化延迟、吞吐量和内存模型推论的占用率。
    3. 对于流水线并行，它将模型按层划分，可以优化吞吐量和内存占用。除延迟外，还可以优化内存占用。
   
    4. 图1说明了单个转化器层的结构。基本上，一个转化器层是由一个多头关注模块和一个mlp模块组成的。和一个mlp模块。灰色块代表中间的结果，即激活，浅色块代表参数，深色块代表查询、键和值。注意力模块。多头注意力模块包括一对线性层和一个多头注意结构。Mlp模块只包括一对线性层。
    ![](/img/in-post/post-ai/inference/single_transformer_layer.png)

* EnergonAI
  
  1. 采用了一个层次控制的系统架构来协调多个 设备并有效地支持不同的并行模式. 
  2. 讲子模型的执行委托给单个controller风格的多个worker，在多个controller的worker中应用张量并行和流水线并行。
  3. 使用了非阻塞并行、分布式冗余计算消除、对等内存池。

  * prons
    * 用户可以使用串行代码方式编写并行代码。
    * 它通过使用更大的异构内存空间，提高了在单个GPU上推断的模型规模。
    * 它通过使用更大的异构存储器空间，以有限的性能降低为代价，提高了在单个GPU上推断的模型规模。

## 应对内存和计算需求的设计
    大型Transformer模型的规模惊人，导致了难以承受的计算和内存需求。因此，与传统的小模型推理系统不同的是系统，必须应用额外的内存管理和通信技术来扩展多GPU上的大型模型在多GPU上的推理系统。

系统设计选择通常是由底层目标硬件的特性和上层工作负载驱动的。
### 性能与可编程性关系

为了追求最佳性能，推理框架的小型transfomer模型，直接使用C++/CUDA实现高度耦合。虽然它可以解决性能相关的问题，但其较差的可编程性一直被诟病。总是受到批评。由于缺乏模块化和抽象化的设计，内存管理和计算被混在一起。混合在一起，使其难以识别一个模块、
比如说一个线性层。例如，在FasterTransformer中有复杂的条件语句，如选择的精度。因此，用户在部署时很难定制。因此，用户很难在部署新模型时定制内存访问和计算模式。

进入大型模型，用户甚至应该把通信逻辑考虑在内，把负担强加在了解HPC相关的知识。在训练系统中，一个神经网络通常被抽象为不同的层。动态语言和绑定技术被用来将内存管理和计算封装在一起作为一个层。这样一来，在我们的EnergonAI中，我们倾向于保留层的概念，将计算、内存管理和通信逻辑封装在一起作为分布式层，以提高可用性。

对于小模型，内核融合技术将两个或多个独立的内核融合为一个单一的内核。将两个或更多的独立内核融合成一个单一的内核，以提高性能，特别是对于这些内存密集型操作。融合后的内核的使用会侵入到到不同的层，破坏了抽象设计。幸运的是、随着模型尺寸激增两个数量级以上、大型Transformer模型的计算功能可以更好地支持该设计。


![](/img/in-post/post-ai/inference/Normalized_kernel_execution.png)
. 图2显示了不同大小的GPT模型在NVIDIA A100 GPU上的归一化不同大小的GPT模型在NVIDIA A100 GPU上的标准化内核执行时间分布。由于单个A100 GPU不能够容纳GPT-66B和GPT-175B、
这里我们采用了它们的配置，并收集了单个转换层的执行在单个变换器层的执行时间。我们选择32作为批次我们选择32作为批处理，64作为序列长度，FP16作为这些实验中的精度。
在这些实验中，我们选择32作为批次大小，64作为序列长度，FP16作为精度，这可以使GPU饱和并达到最大的吞吐量。图2显示图2显示，一般矩阵乘法(GEMM)内核的时间比从62%增加到96%。从62%增加到96%，因为模型的大小从1.25亿到1750亿。

这些计算密集型的内核在大型模型推理中变得更加主要。推理中更占优势。因此，内核融合的动机在很大程度上被削弱。

###  控制流架构
![](/img/in-post/post-ai/inference/controller_flow_architecture.png)

它被抽象为一个两层的架构，即集中式引擎和分布式运行时。分布式运行时是建立在
Pytorch 。它包括一个全局通信环境和一些分布式操作。集中式引擎是为管理分布式多设备推理而新引入的一个抽象层。它负责集中的任务使用远程过程调用（RPC）发布和安排。
因此，集中式引擎实现了一个RPC通信环境。通过精心的封装设计，引擎层使分布式多设备推理的工作得以实现。
引擎层使分布式的多设备推理成为可能。EnergonAI的使用行为与单设备的使用行为相同
推理一样。除了基本架构，还有三种技术来提高计算效率和缓解内存墙问题，即非阻塞流水线
并行性（NBPP）、分布式冗余计算消除（DRCE）和对等内存池（PMEP）。
* 分布式runtime
构建分布式运行时遵循SPMD模型的思想。为了管理通信元信息，例如世界的大小和等级
数，设计了一个全局通信环境。基于基于全局通信上下文，我们实现了张量并行的分布式操作。这些分布式操作包括计算和通信逻辑，执行相关的计算并使用通信来消除数据依赖。对于每个设备来说、它知道它应该计算什么数据，它应该通信，以及它应该与哪个设备通信基于全局通信环境。

* 中心化engine
  
集中式引擎通过RPC管理和控制worker。它主要负责两个独立的阶段，即运行时初始化和执行启动。为了支持这个引擎，在开始时需要一个RPC通信上下文需要在开始时进行初始化。对于
运行时初始化，它将子模型委托给workers、初始化模型的相关部分，将参数加载到内存中，等等。对于执行的启动，它将输入和控制信息给相关的工作器。有了集中式的引擎的集中管理，它可以更有效地执行这些不规则的工作负载。

### 非阻塞Pipeline 并行

* 通过线程池和分布式一致性队列设计，是的异步通信是可行的。
![](/img/in-post/post-ai/inference/engine_management_tp_pp.png)

###  分布式冗余计算的消除

![](/img/in-post/post-ai/inference/drce.png)

###  对等内存池

对等内存池技术将一个节点中的所有内存视为一个整体，并将大型模型的参数存储到大模型的参数存储到内存池中。在内存池中，一个大型模型的参数被分配到不同的GPU中。通过监控所有GPU上的内存空间内存池决定哪个设备可用于来卸载该层。由于CPU和GPU之间的带宽和GPU之间的带宽比NVLINK连接的带宽小得多、只有当我们用尽所有对等的GPU内存时才会使用CPU内存，使用CPU 内存会造成性能严重下降。
![](/img/in-post/post-ai/inference/peer-memory-pool.png)

我们的卸载策略是，要卸载的层是在推理开始前由全局管理者决定的。要卸载的层均匀地分布在那些要保留在设备上的层中。为了实现通信和计算的重叠，一个设备外层的加载过程在前一个层的前向传播开始之前，启动一个非设备层的加载过程。层的前向传播开始之前启动卸载进程，而卸载进程则在计算完成后立即启动。

![](/img/in-post/post-ai/inference/async-layer-prefetching.png)

为了使通信与计算重叠，一个直观的策略是在异步执行前预取一个层。在这里，我们在一个层的执行前预取一定数量的层的参数。确切的层数数量取决于通信和计算的比例，留下一个权衡。此外，必须采用多流模式，以实现层预取的真正异步执行。执行层的预取。在图8的顺序版本中。图8中，阻塞或非阻塞的API只能控制CPU是否启动命令并立即返回。为了实现异步执行，我们需要将Memcpy内核和计算内核安排在不同的CUDA流中。如图8的多流版本、我们启动cudaMemcpyAsync内核来预取第2层的参数，然后再启动这些第0层的计算内核。层0的计算内核。在执行了第0层和第1层之后，我们需要检查流0中的cudaMemcpyAsync内核是否已经完成，或者说第2层的相应参数是否在本地GPU中。
在本地GPU中。

## 参考文件

1. >[ EnergonAI: An Inference System for 10-100 Billion Parameter Transformer Models ]
   (https://arxiv.org/pdf/2209.02341.pdf) 
