---
layout:       post
title:        "LLM 训练那些事情"
author:       "galaxies"
header-style:  text
catalog:      true
tags:
    - LLM Traing
    - OneMoreAI
---

# LLM traing 那些事情

内容来自[OneMoreAI播课(https://media.xyzcdn.net/lurrtqaQT_VmLvz3j3JvZtLVq-Q3.m4a)，

## 当我们讨论大模型时其实是在讨论什么

## 大模型是如何炼成的？
1. Part 2-1：想训大模型？这里有一张入场费账单
   * GPT-1 ，用 A100 需要0.1个卡/年 (1张卡算0.1年）完成一次训练，非常快。卡多一点可能几天就训出来了；
   * GPT-2 就已经到了 6.81个卡/年，这也还好。咱们训练的时候都是八卡甚至更多卡，看起来也是很短时间能训出来的
   * 到了 GPT-3 ，它差不多又大了100倍，所以是400-500个卡/年，大概是这么一个量级。

一次性训出 GPT-1 的成本大概是几千块钱，GPT-2 大概是 30万人民币，训练GPT-3大概是 2500 万人民币。

2. Part 2-2：如何训练大模型效率会更高？
   * 倾向 BF16 ，因为首先 Google 做 T5 一直是拿 BF16，看起来会比较稳定，然后 Hugging Face 去试了一圈以后也是决定 BF16 更好收敛。
   * A100和 A800 其实算力都一样，最大的差别是卡之间的通信带宽，从 600GB/s变成 400GB/s。当我们用上百、上千，甚至上万块显卡做这么大规模训练的时候，通信的主要成本是机器间的通讯。现在就是机器内的通信低了一点。如果我们用 A800，通过更多的工程人员去精心调教并行策略和分布式训练，其实是能够把 200G 的差别给隐藏掉的。
   * 这件事儿对于国内追赶国外，短期来看影响有限，但长期来看可能会非常可怕。H100 的通信带宽会上升到 900GB/s。我们如果还是 400，它是 900，那这是一个非常猛的.更可怕的是 H100 的算力，就是它的 FP16 和 BF16 已经到达了 1979TFLOPs，而 A100、A800 还在 100~300 多 TFLOPs 这个级别。那一下高了四五倍，当然也看具体配置。算力差了六倍，然后通信带宽也差了两倍，那这个其实是非常可怕的。
   * 什么东西在十倍这种量级的差别下都会灰飞烟灭。
   * 分布式训练、模型切分、并行计算：并行计算方案核心还是基于 Megatron 那套框架，Megatron-DeepSpeed 是现在比较 SOTA 的一个方案。
   * 目前的经验：现阶段更倾向于算法人员和工程人员大家彼此知识是交融的，坐下来一起去讨论如何去实现。
   * 我们可以认为大模型的研发未必涉及到很多人，但是它一定是一个非常系统化的工作。

3. Part 2-3：训练中文大语言模型，你的数据够用吗？
* 高质量的内容才是让模型能够达到今天效果的关键。
* 假设我们要在国内做一个千亿量级的大模型，很多人会提到中文数据的质量不如英文数据高，甚至有人认为可用数据中文只有英文的 10% ，这符合大家的认知嘛？当中文数据质量不高的时候，会影响我们去做中文的大语言模型训练吗？
> 我们可以参考 EleutherAI 做的 Pile 数据集。Pile 里面，大家最容易获取到的网页数据占比其实不是很高。剩下的比如说像 arXiv 大量论文、Github、Stackflow、Stack Exchange 等等，我们在中文上好像都很难获取。这导致我们遇到一个困难，我们可能只用了英文集合中很小的一部分。大家可能只能用一些网页数据去做训练，在中文上可能就会有一些问题，比如说它不知道一些最新的 NLP 研究的一些概念，这些概念可能只在 arXiv 上会有
![](/img/in-post/post-ai/data/data-derversity.png)

总结：首先即使是在英文世界里面，能够用来去训练预训练模型的高质量数据其实也不会很多，然后在中文的里面，偏知识型的高质量数据是严重不足的。


4. Part 2-4：训出大模型，人海战术可能并不好使
    训练大语言模型需要哪些人？这些人需要具备什么样的能力呢？
    参考开源团队：去观察 Hugging Face，BigScience 这个项目的人员配置，因为无论是 OpenAI 还是 Meta，他们的论文虽然有，但是人员配置其实不那么透明。我们很难清楚里面的人是干什么的，而 BigScience 整个项目，从头到尾每一个人的身份都是很明确的，大家也能看到这些人是怎么分工的，包括他们每一次的周会也都有录像。
    所以根据 BigScience 的经验，可以总结出几种类型。

   * 数据这块大概就是大数据工程师加少量的法务人员。大数据工程师可能偏数据工作，因为涉及到大量数据的预处理。法务人员可能观察一下比如说数据的 license 是否合理。然后剩下不超过 10 个的 NLP 算法工程师。他们可能更关心模型架构以及训练过程中所有的超参的选型
     
   * 训练：的人可能做一些分布式系统，然后把训练框架给支起来，协调、运维和管理这么多机器。这块可能是有一些算法的经验在里面，但可能更多是系统管理以及软件开发的一些经验。然后可能还需要少量的前后端开发，就是我们前面讲到就是 OpenAI 做 InstuctGPT，其实他们有很多工具在里面，尤其是数据的工具，这里面其实是需要一些前后端开发工作的，然后也包括做像之前 T0 做的 PromptSource工具，所以前后端开发，我猜也会各需要一到两个人。

   不要模仿OPenAI，OpenAI 是远远在前面的，其他所有公司都是追随者。作为追随者，其实需要人会更少，尤其在算法上。因为对我们来讲，OpenAI 那篇 paper 其实都不能说叫 paper，叫算法文档或使用文档，我们照着文档能够模仿出来，这件事情对其他所有公司来说已经是一件很难的事情了，所以我觉得人力规模应该不太需要超过十个人。

   国内状况：数据团队和整个集群管理是自建还是外包可能是影响人员规模的一个重大因素，至于具体的几个人去训练模型，去维护整个分布式系统的可靠，这些反而用的人可能非常少，我猜四五个人就够了。

## Part 3：One More Thing，嘉宾的互问互答

冠叔：开源框架是算法生产的一个非常底层的工具，对于 Transformer 这种架构来讲，未来有没有可能出现一个专有框架，让 Transformer 训起来很爽，或者说在这样一个框架基础之上，它可能会长出非常方便的去训 ChatGPT这样带 instruct tuning、预训练模型和 reward model 这样一套系统的工具？它会不会出现，或者说需要吗？

Q：会出现训练 Transformer 的专用框架吗？
欣然：我理解在不同的层面上其实已经出现了，比如前面反复提到的 Megatron 就是英伟达觉得 Transformer 是下一个时代，所以要做这么一个工具去帮助你很快的做分布式训练。有没有可能会有人把它慢慢做的像 Stable Diffusion 一样，搞成一个开箱即用的工具，然后定向是做 ChatGPT 的，我估计慢慢的肯定会出来的。

龙老师：我比较看好两波。一波就是欣然说的，英伟达肯定会参与其中去推动框架和它的硬件的绑定程度。另外一个需要观察的就是最近Hugging Face 和 AWS 合作。他们其实是做框架或做这种易用性框架的一个好手，在这方面其实已经储备了很多资源，比如说现在大家常用的 Tansformer 框架就是他们开发的。虽然都是一些小模型的，然后包括训练加速，推理加速以及他们在强化学习最近半年做了大量的这种课程，我猜都是在做一些储备。所以我猜未来可能小规模的训练，大家会用 Hugging Face 新出的框架。然后大规模训练用 Megatron 或者英伟达新出的框架。我是这么猜测的。

欣然：其实我还有一个预测。我觉得很明显，英伟达很早之前就往 Transformer 上押注了。他其实是把整个 Transformer 这些特定的结构现在直接做到了它号称比较通用性的芯片里边。

也就是说从H100开始，这个芯片里边直接就有为 Transformer 专门设计的集成电路。英伟达最近三四年往 Transformer 层面一直在努力，所以我预测，如果英伟达没有做一些降智操作的话，很有可能在未来三到五年内，英伟达会希望把整个 Transformer 的小规模的训练能做到单机八卡可以搞定的这种程度，我觉得这是一个非常有可能的事情，那在这种情况下，英伟达就非常有动力去给你做一个非常好用的工具，让人人都能做，我觉得这是一个非常现实的商业战略。

Q：从投资人视角看来，为什么每家都要训出ChatGPT，这合乎逻辑吗？

技术全栈还是需要全端掌握，技术国产化这条路还是需要走的，不能只用舶来品，让他们掌握我们的数据对吧？

Kiwi：这个问题我说说个人的观点，不代表机构。首先，我觉得大语言模型所学习的人类知识以及 ChatGPT 所提供的交互模式，创造了一种全新的人机交互界面。我们回顾科技过去 30 年的发展历史，会发现浏览器的诞生和智能手机的出现分别都创造了一种新的人机交互界面，它大大提高了人类社会对于信息获取、检索和利用的效率，都带来了一波平台性的范式转移。今天让我特别兴奋的，第一当然是大语言模型本身所表现出的学习推理能力，让我对人工智能未来的发展潜力有了更大期待。第二，我认为在这种新的人机交互界面的加持下，人类社会信息获取、信息检索、信息利用、内容生产以及知识创造，都会经历一波非常大的平台性机会。

冠叔：我可以从产品和市场角度做个补充。为什么大家要去追  OpenAI 或者 ChatGPT，因为它相比于 ChatGPT 上层的一些应用，或者说是一些工具厂或者开源项目/开源公司，它的确定性是更高的或者是最高的。大家都很明确一件事情，中国一定需要自己的一个大模型。不管是谁，肯定是有那么一家或者是几家会出来，但是上面比如说像去做开源工具的，或者说一些所谓中间层以及应用层的公司就不一定了。比如开源的项目，可能国内国外它是更互通的，那应用层来讲的话，你不管是去做 ToB 的 SaaS，还是去做一些 ToC 的产品，现在它都有非常高的不确定性。所以从投资人的逻辑来讲，可能就是要找一个非常确定的事情。

Q：ChatGPT 会一统天下吗？

## 参考文件

1. >[ 大模型训练避坑指南，精华整理 ]
   (https://mp.weixin.qq.com/s/s3aPTe11-w_ELL7uZJCI2Q) 
2. >[ 大模型训练避坑指南，播客字幕 ](../srt/one_more_ai/one_more_ai.mp3.srt) 
3. > [The Pile: An 800GB Dataset of Diverse Text for Language Modeling]
(https://arxiv.org/pdf/2101.00027.pdf)