---
layout:     post
title:      "REACT"
subtitle:   " \": SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS\""
date:       2023-04-12 11:20:00
author:     "Galaxies"
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - LLM
    - React
    - reasoning
    - acting
---

> “Yeah It's on. ”


Hux 的 Blog 就这么开通了。

[跳过废话，直接看技术实现 ](#build) 

xxx

<p id = "build"></p>

## ABSTRACT

虽然大型语言模型（LLMs）在语言理解和交互式决策的任务中表现出令人印象深刻的性能，但其推理（如思维链提示）和行动（如行动计划生成）的能力主要是作为独立的课题来研究的。在本文中，我们探索了使用LLMs以交错的方式生成推理痕迹和特定任务的行动，允许两者之间有更大的协同作用：推理痕迹帮助模型诱导、跟踪和更新行动计划以及处理异常，而行动允许它与外部来源（如知识库或环境）对接并收集额外信息。我们将我们的方法（名为ReAct）应用于一组不同的语言和决策任务，并证明了它比最先进的基线更有效，此外还提高了人类的可解释性和可信性。具体来说，在问题回答（HotpotQA）和事实验证（Fever）方面，ReAct通过与简单的维基百科API交互，克服了思维链推理中普遍存在的幻觉和错误传播问题，并产生了类似人类的任务解决轨迹，比没有推理痕迹的基线更容易解释。此外，在两个互动决策基准（ALFWorld和WebShop）上，ReAct胜过模仿和强化学习方法，绝对成功率分别为34%和10%，而提示的只是一个或两个上下文的例子。


## 正文

### INTRODUCTION

人类智力的一个独特特征是能够将面向任务的行动与言语推理（或称内在言语，Alderson-Day & Fernyhough, 2015）无缝结合，这被认为在人类认知中发挥了重要作用，以实现自我调节或策略化（Vygotsky, 1987; Luria, 1965; Fernyhough, 2010）并保持工作记忆（Baddeley, 1992）。考虑一下在厨房里做菜的例子。在任何两个具体的行动之间，我们可能会用语言进行推理，以便跟踪进展（"现在所有的东西都切好了，我应该把锅里的水加热"），处理例外情况或根据情况调整计划（"我没有盐，所以让我用酱油和胡椒粉代替"），并在需要外部信息时意识到（"我如何准备面团？ 让我在网上搜索"）。我们也可能采取行动（打开食谱阅读，打开冰箱，检查成分）来支持推理和回答问题（"我现在可以做什么菜？"）。这种 "行动 "和 "推理 "之间的紧密协同作用使人类能够快速学习新的任务，并进行强有力的决策或推理，即使是在以前没有见过的情况下或面临信息不确定的情况下。
![](/img/in-post/post-ai/llm/app/react.png)
最近的结果暗示了将语言推理与自主系统中的互动决策相结合的可能性。一方面，适当提示的大型语言模型（LLMs）已经证明了在算术、常识和符号推理任务中进行几步推理痕迹以得出答案的新兴能力（Wei等人，2022）。然而，这种 "思维链 "推理是一个静态的黑箱，即模型使用自己的内部表征来产生思维，并不以外部世界为基础，这限制了它反应性推理或更新知识的能力。这可能导致事实幻觉和错误在推理过程中传播等问题（图1（1b））。另一方面，最近的工作探索了使用预先训练好的语言模型在互动环境中计划和行动（Ahn等人，2022；Nakano等人，2021；Yao等人，2020；Huang等人，2022a），重点是通过语言先验预测行动。这些方法通常将多模态观察转换为文本，使用语言模型来生成特定领域的行动或计划，然后使用控制器来选择或执行这些行动。然而，除了Huang等人（2022b）执行有限形式的语言推理以重申关于当前状态的空间事实之外，他们并没有采用语言模型来抽象推理高层次的目标或维持工作记忆以支持行动。除了这种简单的体现任务与几个区块互动之外，还没有研究如何将推理和行动以协同的方式结合起来解决一般的任务，以及与单独的推理或行动相比，这种结合是否能带来系统的好处。

在这项工作中，我们提出了ReAct，一个将推理和行动与语言模型结合起来的一般范式，用于解决不同的语言推理和决策任务（图1）。ReAct提示语言模型以交错的方式产生与任务相关的语言推理痕迹和行动，这使得模型能够进行动态推理，以创建、维护和调整行动的高级计划（推理到行动），同时也与外部环境（例如维基百科）互动，将额外信息纳入推理（行动到推理）。

我们在四个不同的基准上对ReAct和最先进的基准进行了实证评估：问题回答（HotPotQA，Yang等人，2018）、事实验证（Fever，Thorne等人，2018）、基于文本的游戏（ALFWorld，Shridhar等人，2020b）和网页导航（WebShop，Yao等人，2022）。对于HotPotQA和Fever，通过访问模型可以与之互动的维基百科API，ReAct的性能优于普通的行动生成模型，同时与思维链推理（CoT）具有竞争力（Wei等人，2022）。总体而言，最好的方法是ReAct和CoT的结合，允许在推理过程中使用内部知识和外部获得的信息。在ALFWorld和WebShop上，两次甚至一次ReAct提示能够胜过用103∼105个任务实例训练的模仿或强化学习方法，成功率的绝对值分别提高了34%和10%。我们还证明了稀疏的、多功能的推理在决策中的重要性，显示出比仅有行动的控制基线的一致优势。除了普遍适用性和性能提升，推理和行动的结合也有助于提高模型的可解释性、可信度和所有领域的可诊断性，因为人类可以很容易地区分来自模型内部知识和外部环境的信息，以及检查推理痕迹以了解模型行动的决策基础。

总而言之，我们的主要贡献如下： (1) 我们介绍了ReAct，这是一种新型的基于提示的范式，用于协同语言模型中的推理和行为，以解决一般的任务；(2) 我们在不同的基准上进行了广泛的实验，展示了ReAct在几次学习设置中的优势，而之前的方法是孤立地进行推理或行动生成；(3) 我们提出了系统的消解和分析，以了解推理任务中行动和互动任务中推理的重要性；(4) 我们分析了提示设置下ReAct的限制（即对推理和行动的支持有限）。 (4) 我们分析了ReAct在提示设置下的局限性（即对推理和行为的有限支持），并进行了初步的微调实验，表明ReAct有可能通过额外的训练数据来改进。扩大ReAct的规模，在更多的任务上进行训练和操作，并将其与强化学习等补充范式相结合，可以进一步释放大型语言模型的潜力。

---



## 后记



—— 

## 参考文献
1. >[react-lm](https://react-lm.github.io/)